{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series forecasting\n",
    "\n",
    "This summer, I've done an internship in a bank in the City, in London. Time series forcasting was kind of a big deal there and that's why I decided to give it a shot in this Notebook. I should also recognise that during this internship, I've met a former PHD student in machine learning from Imperial College that introduced me to a couple of things concerning time serie machine learning\n",
    "\n",
    "Firstly, we will considere univariate time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the time series into multiple output  for the LSTM model\n",
    "from numpy import array\n",
    "\n",
    "def split_sequence(seq, n_steps):\n",
    "    X,y = [],[]\n",
    "    # a row of X will contain n elements and the correponding row in y will contain the n+1 element\n",
    "    for i in range(len(seq)):\n",
    "        # find the n+1 element of the row\n",
    "        end_ix= i + n_steps\n",
    "        # check if we are not going too far in the sequence\n",
    "        if end_ix > len(seq)-1:\n",
    "            break\n",
    "        # We then build X and y\n",
    "        seq_x , seq_y = seq[i:end_ix], seq[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] 40\n",
      "[20 30 40] 50\n",
      "[30 40 50] 60\n",
      "[40 50 60] 70\n",
      "[50 60 70] 80\n",
      "[60 70 80] 90\n",
      "[70 80 90] 100\n",
      "[ 80  90 100] 110\n",
      "[ 90 100 110] 120\n",
      "[100 110 120] 130\n",
      "[110 120 130] 140\n",
      "[120 130 140] 150\n",
      "[130 140 150] 160\n",
      "[140 150 160] 170\n",
      "[150 160 170] 180\n",
      "[160 170 180] 190\n"
     ]
    }
   ],
   "source": [
    "# Test the function:\n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla LSTM\n",
    "Let's devellop an LSTM model that has a single hidden layer of LSTM units, and an ouput layer used to make prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using univariate time series, we have only one feature in the time serie. Thus, the output of split_sequence (which will be the input of the LSTM model) has the shape:\n",
    "<br/>\n",
    "    ```[samples, timesteps]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit our model with the Adam version of the gradient descent <br/>\n",
    "The loss function used is the mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5721355610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is fit, we can use it to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200.39282]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([170, 180, 190]) # we are expecting to get 100\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM\n",
    "When multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "# return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57182b86d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200.53333]]\n"
     ]
    }
   ],
   "source": [
    "# prediction of the Stacked LSTM\n",
    "\n",
    "x_input = array([170, 180, 190]) # we are expecting to get 100\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "Here the LSTM model will learn the input sequence both forward and backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this for univariate time series, we can wrap the first hidden layer in a wrapper layer called Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "from keras.layers import Bidirectional\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the efficiency of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57082f13d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200.28166]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([170, 180, 190])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM\n",
    "The CNN might be effective to extract feature from a one-dimensional sequence such as univariate time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to use an hybrid system. The idea is the following. The CNN is used to interpret the subsequences (sub divisions of the rows of X) thant are provided together as an input to an LSTM model to interpret. This hybrid model is called CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have to split the input sequences into sub-sequences in order to be processed by the CNN model. <br/>\n",
    "If we take a row of X we already have (if we use 4 steps):<br/>\n",
    "```[30,40,50,60]```\n",
    "\n",
    "\n",
    "We split it to get 2 subsequences:<br/>\n",
    "```[30,40]``` and ```[50,60]```<br/>\n",
    "\n",
    "\n",
    "The CNN will then interpret 2 subsequences of 2 time steps and provide a time series of interpretations of the subsequencies to the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the same CNN model in each subsequence (taken separately) we can wrap the entire CNN model in a TimeDistributed wrapper that apply the model one per input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model :\n",
    "\n",
    "\n",
    "First there is a convolutional layer to read across the subsequence.<br/>\n",
    "number of filter: number of reads/interpretations of the input sequence.<br/>\n",
    "kernel size: number of time steps included of each 'read' operation of the input sequence.\n",
    "\n",
    "\n",
    "Then there is a max pooling mayer, it takes the most salient features of the filters.\n",
    "\n",
    "\n",
    "Finally, we flatten down the structures to a 1D vector to be used by the LSTM layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/benoit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "#then the LSTM model\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56ef5befd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[201.68326]]\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "x_input = array([160, 170, 180, 190])\n",
    "x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM\n",
    "\n",
    "\n",
    "This code is inpired by  [this Medium introduction to ConvLSTM](https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7)\n",
    "\n",
    "\n",
    "ConvLSTM is related to the CNN-LSTM. In ConvLSTM, a convolutional reading of input is built directly in each LSTM unit. It was originally developped to deal with 2D spatial-temporal data. Thus we have to adapt it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer takes as an input a sequence of 2D images. The shape of data must be:\n",
    "\n",
    "\n",
    "```[samples, timesteps, rows, columns, features]```\n",
    "\n",
    "for what we want to do, we can split each row of X into subsequences where timesteps will become the number of subsequences and columns will become the number of time steps for each subsequence. Since we are dealiong with 1D data, the number of row will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the ConvLSTM layer as a single layer in terms of the number of filters and a 2 dimensional kernel size which is (number_rows, number_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters = 64, kernel_size = (1,2), activation = 'relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = 'adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56ee8b9d90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs = 500, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200.82155]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([160, 170, 180, 190])\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate LSM models\n",
    "Here, there is more than 1 observation for each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple input series\n",
    "\n",
    "\n",
    "Here, we want to use multiple imput time series to predict one output time serie thqt is dependant on the input ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "#define input sequences\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  15  25]\n",
      " [ 20  25  45]\n",
      " [ 30  35  65]\n",
      " [ 40  45  85]\n",
      " [ 50  55 105]\n",
      " [ 60  65 125]\n",
      " [ 70  75 145]\n",
      " [ 80  85 165]\n",
      " [ 90  95 185]]\n"
     ]
    }
   ],
   "source": [
    "# reshape data\n",
    "from numpy import hstack\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to structure the data into samples with inputs and output elements, taking care of how LSTM deals with parallel input time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "# kind of similar to split_sequence but we will have to discard the firsts output time serie value\n",
    "\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # build the input and output\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix -1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3, 2) (7,)\n",
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] 65\n",
      "[[20 25]\n",
      " [30 35]\n",
      " [40 45]] 85\n",
      "[[30 35]\n",
      " [40 45]\n",
      " [50 55]] 105\n",
      "[[40 45]\n",
      " [50 55]\n",
      " [60 65]] 125\n",
      "[[50 55]\n",
      " [60 65]\n",
      " [70 75]] 145\n",
      "[[60 65]\n",
      " [70 75]\n",
      " [80 85]] 165\n",
      "[[70 75]\n",
      " [80 85]\n",
      " [90 95]] 185\n"
     ]
    }
   ],
   "source": [
    "# test it on our dataset\n",
    "\n",
    "n_steps = 3\n",
    "\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])\n",
    "\n",
    "n_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation = 'relu', input_shape = (n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.677895]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction\n",
    "x_input = array([[80, 85], [90, 95], [100, 105]])\n",
    "# we expect 205\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[206.05481]]\n"
     ]
    }
   ],
   "source": [
    "# multivariate lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([[80, 85], [90, 95], [100, 105]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
